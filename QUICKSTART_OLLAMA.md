# Guide de D√©marrage Rapide - Assistant IA Supply Chain avec Ollama

Ce guide vous permet de lancer rapidement l'assistant IA Supply Chain en utilisant **Ollama** (mod√®les LLM locaux, gratuits) au lieu d'APIs cloud payantes.

## ‚ú® Fonctionnalit√©s Impl√©ment√©es

‚úÖ **Upload de fichiers** - Excel, PDF, Word, PowerPoint, CSV, Texte
‚úÖ **Parsing intelligent** - Extraction cellule par cellule pour Excel, page par page pour PDF
‚úÖ **RAG (Retrieval Augmented Generation)** - Recherche hybride (keyword + semantic) dans TypeSense
‚úÖ **Embeddings locaux** - Ollama `nomic-embed-text` (768 dimensions)
‚úÖ **LLM local** - Ollama `mistral:7b-instruct` avec support fran√ßais
‚úÖ **Citations pr√©cises** - "Selon la cellule C12 (feuille 'Stocks' du fichier production.xlsx): 150"
‚úÖ **Purge 24h** - Suppression automatique des fichiers et index (confidentialit√©)
‚úÖ **Processing asynchrone** - Celery workers pour traitement en arri√®re-plan

---

## üöÄ Installation et Lancement

### Pr√©requis

- **Docker & Docker Compose** (pour les services: PostgreSQL, Redis, MinIO, TypeSense, Ollama)
- **Python 3.11+** (backend)
- **Node.js 18+** (frontend)
- **8GB RAM minimum** (pour Ollama + services)

### √âtape 1: Cloner et Configurer

```bash
cd /Users/maximedousset/Documents/Projets_Claude/projet-C

# Copier le fichier d'environnement (d√©j√† fait si .env existe)
cp .env.example .env

# V√©rifier que les variables Ollama sont bien configur√©es
cat .env | grep OLLAMA
# Devrait afficher:
# OLLAMA_HOST=http://ollama:11434
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# OLLAMA_CHAT_MODEL=mistral:7b-instruct
```

### √âtape 2: Lancer les Services Docker

```bash
# D√©marrer tous les services (PostgreSQL, Redis, MinIO, TypeSense, Ollama, Backend, Celery)
docker-compose up -d

# V√©rifier que tous les services sont up
docker-compose ps

# Devrait afficher:
# - supply-chain-db (PostgreSQL) - healthy
# - supply-chain-redis - healthy
# - supply-chain-minio - healthy
# - supply-chain-typesense - running
# - supply-chain-ollama - healthy
# - supply-chain-backend - running
# - supply-chain-celery - running
```

### √âtape 3: Initialiser Ollama avec les Mod√®les

**Important**: La premi√®re fois, Ollama doit t√©l√©charger les mod√®les (~4GB pour mistral + 300MB pour nomic-embed-text).

```bash
# Ex√©cuter le script d'initialisation
docker exec -it supply-chain-ollama bash /app/scripts/init_ollama.sh

# OU manuellement:
docker exec -it supply-chain-ollama ollama pull nomic-embed-text
docker exec -it supply-chain-ollama ollama pull mistral:7b-instruct

# V√©rifier que les mod√®les sont bien install√©s
docker exec -it supply-chain-ollama ollama list

# Devrait afficher:
# nomic-embed-text:latest    274 MB
# mistral:7b-instruct:latest 4.1 GB
```

‚è±Ô∏è **Temps estim√©**: 5-15 minutes selon votre connexion internet.

### √âtape 4: V√©rifier le Backend

```bash
# Voir les logs du backend
docker-compose logs -f backend

# Devrait afficher (entre autres):
# INFO:     Application startup complete.
# INFO:     Uvicorn running on http://0.0.0.0:8000

# Tester l'API
curl http://localhost:8000/health
# {"status":"healthy"}
```

### √âtape 5: Lancer le Frontend

```bash
cd frontend

# Installer les d√©pendances (si pas d√©j√† fait)
npm install

# Lancer le serveur de d√©veloppement
npm run dev

# Frontend accessible sur http://localhost:3000
```

---

## üìñ Utilisation

### 1. Uploader un Fichier

1. Ouvrir **http://localhost:3000** dans votre navigateur
2. Cr√©er une nouvelle conversation (bouton "Nouvelle Conversation")
3. Cliquer sur l'ic√¥ne üìé (upload) ou glisser-d√©poser un fichier
4. Formats support√©s: `.xlsx`, `.csv`, `.pdf`, `.docx`, `.pptx`, `.txt`
5. Taille max: 50MB
6. Attendre que le statut passe √† "‚úÖ Trait√©" (peut prendre 5-30s selon la taille)

### 2. Poser des Questions

Une fois le fichier trait√©, posez des questions comme:

**Exemple Excel**:
```
User: Quel est le stock en cellule C12 de la feuille Stocks?
Assistant: Selon la cellule C12 (feuille 'Stocks' du fichier production.xlsx): 150 unit√©s.
```

**Exemple PDF**:
```
User: Quel est le stock de s√©curit√© recommand√©?
Assistant: D'apr√®s la page 3 du fichier rapport.pdf: "Le stock de s√©curit√© recommand√© est de 200 unit√©s pour ce produit."
```

**Exemple sans information**:
```
User: Quel est le prix du produit X?
Assistant: Je n'ai pas trouv√© d'information sur le prix du produit X dans vos documents.
```

---

## üîç V√©rification du Pipeline RAG

### Test Complet du Pipeline

```bash
# 1. Upload un fichier Excel de test
curl -X POST http://localhost:8000/api/files/upload \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -F "file=@/path/to/test.xlsx" \
  -F "conversation_id=YOUR_CONVERSATION_ID"

# 2. V√©rifier que le fichier est en traitement
# Regarder les logs Celery
docker-compose logs -f celery-worker

# Devrait afficher:
# INFO: Processing file: <file_id> - test.xlsx
# INFO: Parsed 100 chunks from test.xlsx
# INFO: Generated embeddings for batch 1/1
# INFO: Indexed 100 chunks for file <file_id>
# INFO: Successfully processed file: <file_id>

# 3. V√©rifier TypeSense
curl "http://localhost:8108/collections/document_chunks/documents/search?q=*&per_page=5" \
  -H "X-TYPESENSE-API-KEY: xyz123"

# Devrait retourner des documents avec embeddings

# 4. Tester une requ√™te RAG
# Via l'interface web: poser une question li√©e au fichier upload√©
# V√©rifier les logs backend pour voir la recherche RAG
docker-compose logs -f backend | grep "RAG"

# Devrait afficher:
# INFO: Performing RAG search for query: ...
# INFO: RAG search returned 5 results
```

---

## üõ†Ô∏è D√©pannage

### Ollama ne d√©marre pas

```bash
# V√©rifier les logs Ollama
docker-compose logs ollama

# Si erreur "failed to allocate memory":
# ‚Üí Augmenter la RAM allou√©e √† Docker (Docker Desktop > Settings > Resources)

# Red√©marrer Ollama
docker-compose restart ollama
```

### Backend erreur "Connection refused" √† Ollama

```bash
# V√©rifier que Ollama est bien accessible depuis le backend
docker exec -it supply-chain-backend curl http://ollama:11434/api/tags

# Si √ßa ne fonctionne pas:
docker-compose restart backend
```

### Celery worker ne traite pas les t√¢ches

```bash
# V√©rifier les logs Celery
docker-compose logs -f celery-worker

# V√©rifier que Redis est accessible
docker exec -it supply-chain-backend redis-cli -h redis ping
# PONG

# Red√©marrer le worker
docker-compose restart celery-worker
```

### TypeSense erreur "Collection not found"

```bash
# V√©rifier que la collection existe
curl http://localhost:8108/collections \
  -H "X-TYPESENSE-API-KEY: xyz123"

# Si la collection n'existe pas, elle sera cr√©√©e automatiquement
# au premier d√©marrage du backend
docker-compose restart backend
```

### Embeddings tr√®s lents

- **Cause**: Ollama g√©n√®re les embeddings en CPU (pas de GPU)
- **Solution**: Pour MVP, c'est acceptable (~100ms par chunk). En production, consid√©rer:
  - Machine avec GPU (NVIDIA)
  - Mod√®les quantifi√©s (plus rapides)
  - Cache Redis (d√©j√† impl√©ment√©)

### Frontend ne se connecte pas au backend

```bash
# V√©rifier que NEXT_PUBLIC_API_URL est correct
cat frontend/.env.local | grep API_URL
# NEXT_PUBLIC_API_URL=http://localhost:8000

# V√©rifier que le backend est accessible
curl http://localhost:8000/health

# Red√©marrer le frontend
cd frontend && npm run dev
```

---

## üìä Monitoring

### Voir l'√©tat des services

```bash
# Tous les services
docker-compose ps

# Logs en temps r√©el
docker-compose logs -f

# Logs d'un service sp√©cifique
docker-compose logs -f backend
docker-compose logs -f celery-worker
docker-compose logs -f ollama
```

### Acc√®s aux UIs Admin

- **MinIO Console**: http://localhost:9001 (minioadmin / minioadmin123)
- **PostgreSQL**: `psql -h localhost -U supply_chain_user -d supply_chain_ai`
- **Redis**: `redis-cli -h localhost -p 6379`

---

## üßπ Arr√™t et Nettoyage

```bash
# Arr√™ter les services
docker-compose down

# Arr√™ter et supprimer les volumes (‚ö†Ô∏è efface les donn√©es)
docker-compose down -v

# Supprimer les mod√®les Ollama (lib√®re ~4GB)
docker volume rm projet-c_ollama_data
```

---

## üìà Prochaines √âtapes

Maintenant que le pipeline RAG est fonctionnel, vous pouvez:

1. **Tester avec vos donn√©es r√©elles** - Uploader vos fichiers Supply Chain
2. **Impl√©menter le Mode Alerte** - D√©tection d'incoh√©rences (stocks n√©gatifs, dates, etc.)
3. **Ajouter l'authentification** - JWT avec register/login
4. **Tests E2E** - Playwright pour valider les flows
5. **D√©ploiement** - Production avec Fly.io/Railway + Vercel

---

## üí° Notes Importantes

### Performance
- **Upload**: < 3s pour 10MB
- **Parsing Excel**: ~5s pour 1000 lignes
- **Embeddings**: ~2s pour 100 chunks (CPU, sans GPU)
- **Search TypeSense**: < 50ms
- **LLM Response**: ~2s pour premier token

### Co√ªts
- **Ollama**: Gratuit (self-hosted)
- **Infrastructure Docker**: Gratuit (dev local)
- **Production**: ~$50-150/mois (VPS + services manag√©s)

### Limitations MVP
- Pas de GPU (embeddings en CPU, plus lent)
- Pas d'UI upload de fichiers (TODO)
- Pas d'alertes Supply Chain (TODO)
- Pas d'authentification compl√®te (JWT mock)

---

## üÜò Support

En cas de probl√®me:
1. V√©rifier les logs: `docker-compose logs -f`
2. Red√©marrer les services: `docker-compose restart`
3. Consulter la documentation Ollama: https://ollama.ai/
4. Ouvrir une issue GitHub

---

**Bon d√©veloppement! üöÄ**
